{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras import callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import utils\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAME_TRAIN = r'C:\\Users\\Annie\\Documents\\Working\\Корпус биографических текстов\\Данные для обучения сетей\\oversampling\\ALL.csv'\n",
    "NAME_TEST = r'C:\\Users\\Annie\\Documents\\Working\\Корпус биографических текстов\\Данные для обучения сетей\\oversampling\\TEST.csv'\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_NEURONS = 1024\n",
    "LAYERS = 2\n",
    "ACTIVATION = \"tanh\"\n",
    "DROPOUT = 0.5\n",
    "EARLY_STOPPING = 5\n",
    "MODEL_PATH = \"1.hdf5\"\n",
    "GAMMA = 0.1\n",
    "N_NEIGH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(NAME_TRAIN) as f:\n",
    "        df = pd.read_csv(f, sep=';', header=None, decimal = '.')\n",
    "\n",
    "train_data = df.values\n",
    "random.seed(139)  # для воспроизводимость\n",
    "random.shuffle(train_data)\n",
    "m = len(train_data)\n",
    "\n",
    "train_length = int(0.8 * m)\n",
    "train_data, test_data = train_data[:train_length],\\\n",
    "    train_data[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data)\n",
    "texts = df[0].tolist()\n",
    "categories = df[1].tolist()\n",
    "vectors = df.drop([0,1], axis=1).values\n",
    "\n",
    "df = pd.DataFrame(test_data)\n",
    "texts_test = df[0].tolist()\n",
    "categories_test = df[1].tolist()\n",
    "vectors_test = df.drop([0,1], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#не запускать\n",
    "with open(NAME_TRAIN) as f:\n",
    "        df = pd.read_csv(f, sep=';', header=None, decimal = '.')\n",
    "with open(NAME_TEST) as f:\n",
    "        df_test = pd.read_csv(f, sep=';', header=None, decimal = '.')\n",
    "texts = df[0].tolist()\n",
    "categories = df[1].tolist()\n",
    "vectors = df.drop([0,1], axis=1).values\n",
    "texts_test = df_test[0].tolist()\n",
    "categories_test = df_test[1].tolist()\n",
    "vectors_test = df_test.drop([0,1], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample:\n",
      "[191. 165. 319. 516. 171.  44.  15. 118.  23.  15.]\n",
      "count of sentences: 1577.0\n",
      "max class: 516.0\n",
      "test sample:\n",
      "[ 26.  21. 127.  67.  29.   6.   8.  81.  15.  15.]\n"
     ]
    }
   ],
   "source": [
    "types_of_facts  = {\"birth\": 0, \"death\": 1, \"occupation\": 2, \"education\": 3, \"affiliation\": 4, \"family\": 5, \\\n",
    "         \"parenting\": 6, \"professional_events\": 7, \"personal_events\": 8, \"residence\": 9}\n",
    "\n",
    "print(\"training sample:\")\n",
    "num_classes = len(types_of_facts)\n",
    "count = np.zeros(shape=(num_classes))\n",
    "for i in range(len(categories)):\n",
    "    categories[i] = types_of_facts[categories[i]]\n",
    "    count[categories[i]]+=1\n",
    "print(count)\n",
    "print(\"count of sentences: {0}\".format(count.sum()))\n",
    "M=count.max()\n",
    "print(\"max class: {0}\".format(M))\n",
    "\n",
    "print(\"test sample:\")\n",
    "count_test = np.zeros(shape=(num_classes))\n",
    "for i in range(len(categories_test)):\n",
    "    categories_test[i] = types_of_facts[categories_test[i]]\n",
    "    count_test[categories_test[i]]+=1\n",
    "print(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08885499509051442 -0.07851399946957828 0.2726290060672909 ...\n",
      "  -0.12033999804407358 0.1860189912840724 0.3426859998144209]\n",
      " [0.08885499509051442 -0.07851399946957828 0.2726290060672909 ...\n",
      "  -0.12033999804407358 0.1860189912840724 0.3426859998144209]\n",
      " [0.15586599835660309 -0.004857996478676796 0.06730599910952151 ...\n",
      "  -0.2980579999275505 0.5592820029705763 -0.31733799912035465]\n",
      " ...\n",
      " [0.09618299768771976 -0.061186004430055625 0.1815540031529963 ...\n",
      "  -0.4477570094168186 0.10406100004911424 -0.17310800217092034]\n",
      " [0.015114001929759981 0.08529199712211266 0.18716900423169136 ...\n",
      "  -0.3612539973109961 0.44122800789773453 0.0526020023971796]\n",
      " [0.2685830066911876 -0.1521739938762039 0.3618329963646829 ...\n",
      "  -0.18147299624979496 0.6014230037108064 -0.060259000398218625]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(types_of_facts['birth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    text = ' '.join(morph.parse(word)[0].normal_form for word in text.split())\n",
    "    return text\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = transform_text(texts[i])\n",
    "for i in range(len(texts_test)):\n",
    "    texts_test[i] = transform_text(texts_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birth\n",
      "death\n",
      "occupation\n",
      "education\n",
      "affiliation\n",
      "family\n",
      "parenting\n",
      "professional_events\n",
      "personal_events\n",
      "residence\n"
     ]
    }
   ],
   "source": [
    "#sampling word2vec Xu Chen Word Embedding Composition for Data Imbalances in Sentiment and Emotion Classification\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def get_key(d, value):\n",
    "    for k, v in d.items():\n",
    "        if v == value:\n",
    "            return k\n",
    "\n",
    "for type_fact in types_of_facts:\n",
    "    #new_cat=[]\n",
    "    new_vec=[]\n",
    "    count=0\n",
    "    print(type_fact)\n",
    "    for i in range(len(categories)):\n",
    "        if get_key(types_of_facts,categories[i])==type_fact:\n",
    "            count+=1\n",
    "    new_vec=np.zeros(shape=(count,300))\n",
    "    k=0\n",
    "    for i in range(len(categories)):\n",
    "        if get_key(types_of_facts,categories[i])==type_fact:\n",
    "            new_vec[k]=vectors[i]\n",
    "            k+=1    \n",
    "    \n",
    "    new_count=count\n",
    "    #while count<516:\n",
    "    while count<sampling_strategy[types_of_facts[type_fact]]:\n",
    "        alpha=random.random()\n",
    "        num1=random.randint(0,new_count-1)\n",
    "        num2=random.randint(0,new_count-1)\n",
    "        new_elem=np.zeros(shape=(1,300))\n",
    "        for j in range(len(new_vec[num1])):\n",
    "            new_elem[0][j]=new_vec[num1][j]-alpha*new_vec[num2][j]\n",
    "\n",
    "        vectors= np.append(vectors,new_elem,axis=0)\n",
    "\n",
    "        categories=np.append(categories,types_of_facts[type_fact])\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5160"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[354. 340. 418. 516. 344. 280. 266. 317. 270. 266.]\n",
      "count of sentences: 3371.0\n",
      "max class: 516.0\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(types_of_facts)\n",
    "count = np.zeros(shape=(num_classes))\n",
    "for i in range(len(categories)):\n",
    "    count[categories[i]]+=1\n",
    "print(count)\n",
    "print(\"count of sentences: {0}\".format(count.sum()))\n",
    "M=count.max()\n",
    "print(\"max class: {0}\".format(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'can', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "sw_list = get_stop_words('russian')+get_stop_words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=sw_list)\n",
    "all_words = vectorizer.fit_transform(texts)\n",
    "bag_of_words = all_words.toarray()\n",
    "all_words = vectorizer.transform(texts_test)\n",
    "bag_of_words_test = all_words.toarray()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sw_list)\n",
    "all_words = vectorizer.fit_transform(texts)\n",
    "tfidf = all_words.toarray()\n",
    "all_words = vectorizer.transform(texts_test)\n",
    "tfidf_test = all_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 354, 1: 340, 2: 418, 3: 516, 4: 344, 5: 280, 6: 266, 7: 317, 8: 270, 9: 266}\n"
     ]
    }
   ],
   "source": [
    "k_parameter = 0.5\n",
    "\n",
    "sampling_strategy = {0: int(round((M-count[0])*k_parameter+count[0])), 1: int(round((M-count[1])*k_parameter+count[1])), 2: int(round((M-count[2])*k_parameter+count[2])), \\\n",
    "                     3: int(round((M-count[3])*k_parameter+count[3])), 4: int(round((M-count[4])*k_parameter+count[4])), 5: int(round((M-count[5])*k_parameter+count[5])), \\\n",
    "                     6: int(round((M-count[6])*k_parameter+count[6])), 7: int(round((M-count[7])*k_parameter+count[7])), 8: int(round((M-count[8])*k_parameter+count[8])), \\\n",
    "                     9: int(round((M-count[9])*k_parameter+count[9]))}\n",
    "\n",
    "print(sampling_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling_strategy = 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=42, sampling_strategy=sampling_strategy)\n",
    "#vectors_ros, categories_ros = ros.fit_resample(bag_of_words, categories)\n",
    "#vectors_ros, categories_ros = ros.fit_resample(tfidf, categories)\n",
    "vectors_ros, categories_ros = ros.fit_resample(vectors, categories)\n",
    "print(len(vectors_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42, sampling_strategy=sampling_strategy)\n",
    "#vectors_sm, categories_sm = sm.fit_resample(bag_of_words, categories)\n",
    "#vectors_sm, categories_sm = sm.fit_resample(tfidf, categories)\n",
    "vectors_sm, categories_sm = sm.fit_resample(vectors, categories)\n",
    "print(len(vectors_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n"
     ]
    }
   ],
   "source": [
    "bsm = BorderlineSMOTE(random_state=42, sampling_strategy=sampling_strategy)\n",
    "#vectors_bsm, categories_bsm = bsm.fit_resample(bag_of_words, categories)\n",
    "#vectors_bsm, categories_bsm = bsm.fit_resample(tfidf, categories)\n",
    "vectors_bsm, categories_bsm = bsm.fit_resample(vectors, categories)\n",
    "print(len(vectors_bsm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293\n"
     ]
    }
   ],
   "source": [
    "adasyn = ADASYN(random_state=42, sampling_strategy=sampling_strategy)\n",
    "#vectors_as, categories_as = adasyn.fit_resample(bag_of_words, categories)\n",
    "#vectors_as, categories_as = adasyn.fit_resample(tfidf, categories)\n",
    "vectors_as, categories_as = adasyn.fit_resample(vectors, categories)\n",
    "print(len(vectors_as))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.063291\n",
      "F1-score: 60.150465\n",
      "Precision: 69.249918\n",
      "Recall: 59.182503\n",
      "[0.75       0.76       0.6728972  0.74853801 0.69047619 0.66666667\n",
      " 0.27272727 0.47368421 0.46153846 0.51851852]\n",
      "[0.92307692 0.9047619  0.56692913 0.95522388 1.         1.\n",
      " 0.375      0.33333333 0.4        0.46666667]\n",
      "[0.63157895 0.65517241 0.82758621 0.61538462 0.52727273 0.5\n",
      " 0.21428571 0.81818182 0.54545455 0.58333333]\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 5)\n",
    "neigh.fit(vectors, categories) \n",
    "predict = neigh.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))\n",
    "print(f1_score(predict, categories_test, average=None))\n",
    "print(precision_score(predict, categories_test, average=None))\n",
    "print(recall_score(predict, categories_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.392405\n",
      "F1-score: 66.936215\n",
      "Precision: 71.917279\n",
      "Recall: 68.005393\n",
      "[0.77419355 0.85714286 0.74235808 0.79012346 0.75324675 0.75\n",
      " 0.5        0.57142857 0.41666667 0.53846154]\n",
      "[0.92307692 1.         0.66929134 0.95522388 1.         1.\n",
      " 0.375      0.4691358  0.33333333 0.46666667]\n",
      "[0.66666667 0.75       0.83333333 0.67368421 0.60416667 0.6\n",
      " 0.75       0.73076923 0.55555556 0.63636364]\n",
      "Accuracy: 71.645570\n",
      "F1-score: 66.368617\n",
      "Precision: 73.821294\n",
      "Recall: 63.904952\n",
      "[0.83333333 0.88888889 0.7281106  0.82894737 0.75324675 0.66666667\n",
      " 0.46153846 0.62318841 0.35294118 0.5       ]\n",
      "[0.96153846 0.95238095 0.62204724 0.94029851 1.         1.\n",
      " 0.375      0.5308642  0.4        0.6       ]\n",
      "[0.73529412 0.83333333 0.87777778 0.74117647 0.60416667 0.5\n",
      " 0.6        0.75438596 0.31578947 0.42857143]\n",
      "Accuracy: 68.354430\n",
      "F1-score: 63.993207\n",
      "Precision: 70.795378\n",
      "Recall: 67.168878\n",
      "[0.78688525 0.82608696 0.68493151 0.84       0.64285714 0.57142857\n",
      " 0.54545455 0.54929577 0.38095238 0.57142857]\n",
      "[0.92307692 0.9047619  0.59055118 0.94029851 0.93103448 1.\n",
      " 0.375      0.48148148 0.26666667 0.66666667]\n",
      "[0.68571429 0.76       0.81521739 0.75903614 0.49090909 0.4\n",
      " 1.         0.63934426 0.66666667 0.5       ]\n",
      "Accuracy: 75.189873\n",
      "F1-score: 71.529685\n",
      "Precision: 77.623502\n",
      "Recall: 69.820804\n",
      "[0.87719298 0.84       0.7860262  0.88732394 0.73417722 0.8\n",
      " 0.61538462 0.60431655 0.44444444 0.56410256]\n",
      "[0.96153846 1.         0.70866142 0.94029851 1.         1.\n",
      " 0.5        0.51851852 0.4        0.73333333]\n",
      "[0.80645161 0.72413793 0.88235294 0.84       0.58       0.66666667\n",
      " 0.8        0.72413793 0.5        0.45833333]\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(vectors_ros, categories_ros) \n",
    "predict = neigh.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))\n",
    "print(f1_score(predict, categories_test, average=None))\n",
    "print(precision_score(predict, categories_test, average=None))\n",
    "print(recall_score(predict, categories_test, average=None))\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(vectors_sm, categories_sm) \n",
    "predict = neigh.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))\n",
    "print(f1_score(predict, categories_test, average=None))\n",
    "print(precision_score(predict, categories_test, average=None))\n",
    "print(recall_score(predict, categories_test, average=None))\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(vectors_bsm, categories_bsm) \n",
    "predict = neigh.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))\n",
    "print(f1_score(predict, categories_test, average=None))\n",
    "print(precision_score(predict, categories_test, average=None))\n",
    "print(recall_score(predict, categories_test, average=None))\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "neigh.fit(vectors_as, categories_as) \n",
    "predict = neigh.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))\n",
    "print(f1_score(predict, categories_test, average=None))\n",
    "print(precision_score(predict, categories_test, average=None))\n",
    "print(recall_score(predict, categories_test, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.772152\n",
      "F1-score: 76.946060\n",
      "Precision: 75.630705\n",
      "Recall: 87.110306\n"
     ]
    }
   ],
   "source": [
    "#clf = svm.LinearSVC()\n",
    "clf = svm.SVC(gamma = GAMMA)\n",
    "clf.fit(vectors, categories) \n",
    "predict = clf.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.037975\n",
      "F1-score: 79.249732\n",
      "Precision: 76.374927\n",
      "Recall: 90.194547\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(gamma = GAMMA)\n",
    "clf.fit(vectors_sm, categories_sm) \n",
    "predict = clf.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.025316\n",
      "F1-score: 77.752071\n",
      "Precision: 74.452982\n",
      "Recall: 90.725933\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(gamma = GAMMA)\n",
    "clf.fit(vectors_bsm, categories_bsm) \n",
    "predict = clf.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.797468\n",
      "F1-score: 79.619753\n",
      "Precision: 76.434494\n",
      "Recall: 91.319114\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(gamma = GAMMA)\n",
    "clf.fit(vectors_as, categories_as) \n",
    "predict = clf.predict(vectors_test)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, categories_test)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, categories_test, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, categories_test, average=\"macro\")*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = bag_of_words\n",
    "y_train = keras.utils.to_categorical(categories, num_classes)\n",
    "x_test = bag_of_words_test\n",
    "y_test = keras.utils.to_categorical(categories_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1577 samples, validate on 395 samples\n",
      "Epoch 1/2\n",
      "1577/1577 [==============================] - 25s 16ms/step - loss: 0.7350 - acc: 0.7806 - val_loss: 0.8060 - val_acc: 0.7671\n",
      "Epoch 2/2\n",
      "1577/1577 [==============================] - 24s 15ms/step - loss: 0.1248 - acc: 0.9645 - val_loss: 0.9984 - val_acc: 0.8000\n",
      "Accuracy: 80.000000\n",
      "F1-score: 73.625253\n",
      "Precision: 72.593262\n",
      "Recall: 82.004576\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(HIDDEN_NEURONS, input_dim=len(x_train[0]), activation=ACTIVATION))\n",
    "model.add(Dropout(DROPOUT))\n",
    "for i in range(LAYERS):\n",
    "    model.add(Dense(HIDDEN_NEURONS, activation=ACTIVATION))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(EarlyStopping(monitor=\"val_acc\", patience=EARLY_STOPPING))\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_PATH, monitor=\"val_acc\",\n",
    "                                       save_best_only=True, save_weights_only=True)\n",
    "callbacks.append(model_checkpoint)\n",
    "model.fit(x_train, y_train, epochs=2, batch_size = BATCH_SIZE, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "\n",
    "predict = np.argmax(model.predict(x_test), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1577 samples, validate on 395 samples\n",
      "Epoch 1/1\n",
      "1577/1577 [==============================] - 25s 16ms/step - loss: 0.0135 - acc: 0.9949 - val_loss: 1.2027 - val_acc: 0.7747\n",
      "Accuracy: 77.468354\n",
      "F1-score: 75.504739\n",
      "Precision: 76.414882\n",
      "Recall: 77.075376\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=1, batch_size = BATCH_SIZE, validation_data=(x_test, y_test), callbacks=callbacks)\n",
    "\n",
    "predict = np.argmax(model.predict(x_test), axis=1)\n",
    "answer = np.argmax(y_test, axis=1)\n",
    "print('Accuracy: %f' % (accuracy_score(predict, answer)*100))\n",
    "print('F1-score: %f' % (f1_score(predict, answer, average=\"macro\")*100))\n",
    "print('Precision: %f' % (precision_score(predict, answer, average=\"macro\")*100))\n",
    "print('Recall: %f' % (recall_score(predict, answer, average=\"macro\")*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors_ros))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
